\relax 
\citation{me}
\citation{inspiration}
\citation{bestprac}
\citation{frac}
\citation{inspiration}
\citation{kernel}
\citation{crop}
\citation{inspiration}
\citation{bestprac}
\citation{frac}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Baseline Model parameters\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:baseline}{{1}{1}}
\citation{frac}
\citation{allconv}
\citation{tf}
\citation{inspiration}
\citation{me}
\citation{kernel}
\citation{crop}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Initial parameters for Convolution and Pooling layers\relax }}{3}}
\newlabel{tab:convpoolpar}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of different architecture model accuracies for 30 epochs, training set on the left and validation on the right. Highlighting the best performing one\relax }}{3}}
\newlabel{fig:arch}{{1}{3}}
\citation{kernel}
\citation{inspiration}
\citation{bestprac}
\citation{stan}
\citation{alexnet}
\citation{kernel}
\citation{inspiration}
\citation{stan}
\citation{frac}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Model performance without random cropping data augmentation showing over-fitting. \relax }}{4}}
\newlabel{fig:noaug}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Model performance with random cropping data augmentation. \relax }}{4}}
\newlabel{fig:aug}{{3}{4}}
\citation{frac}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Model performance on validation set with various filter sizes and stride lengths. \relax }}{5}}
\newlabel{fig:filter}{{4}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of two standard pooling parameters 2x2 and 3x3 with stride 2.\relax }}{5}}
\newlabel{fig:pool}{{5}{5}}
\citation{allconv}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Fractional pooling parameters\relax }}{6}}
\newlabel{tab:fracpool}{{3}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Fractional pooling performance with different parameters and number of layers. \relax }}{6}}
\newlabel{fig:fracpool}{{6}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The All-CNN-C architecture\relax }}{6}}
\newlabel{tab:allcnn}{{7}{6}}
\citation{me}
\citation{inspiration}
\citation{alexnet}
\citation{stan}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of the All-CNN-C and the standard pipeline model explored in this coursework. \relax }}{7}}
\newlabel{fig:al}{{8}{7}}
\citation{kernel}
\citation{crop}
\citation{kernel}
\citation{inspiration}
\citation{bestprac}
\citation{alexnet}
\citation{inspiration}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Comparison of baseline and conovlutional network\relax }}{8}}
\newlabel{fig:netcomp}{{9}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Effects of using random cropping.\relax }}{8}}
\newlabel{fig:augcomp}{{10}{8}}
\citation{inspiration}
\citation{frac}
\citation{frac}
\citation{allconv}
\citation{stan}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Comparison of fractional and max pooling.\relax }}{9}}
\newlabel{fig:fraccomp}{{11}{9}}
\citation{frac}
\citation{allconv}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Comparison of the best performing models explored with baseline.\relax }}{10}}
\newlabel{fig:rescomp}{{12}{10}}
\citation{inspiration}
\citation{alexnet}
\citation{stan}
\citation{kernel}
\citation{inspiration}
\citation{bestprac}
\citation{alexnet}
\citation{frac}
\citation{allconv}
\citation{frac}
\citation{allconv}
\citation{frac}
\bibcite{me}{1}
\bibcite{inspiration}{2}
\bibcite{kernel}{3}
\bibcite{bestprac}{4}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Initial parameters for Convolution and Pooling layers\relax }}{11}}
\newlabel{tab:fracres}{{4}{11}}
\bibcite{frac}{5}
\bibcite{tf}{6}
\bibcite{crop}{7}
\bibcite{stan}{8}
\bibcite{alexnet}{9}
\bibcite{allconv}{10}
