\relax 
\citation{grey1}
\citation{grey2}
\citation{grey3}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Model parameters\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:model}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Effect of pre-porcessing the CIFAR dataset by transforming into different colour spaces on classification performance. Trained using AdamOptimizer with 3 layers of 200 units each.\relax }}{2}}
\newlabel{fig:colour}{{1}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Effect of pre-porcessing the CIFAR dataset by transforming into different colour spaces on training time.\relax }}{2}}
\newlabel{tab:colour}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Classification accuracy comparison on architectures with different numbers of layers and hidden units. \relax }}{3}}
\newlabel{fig:arch}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Classification accuracy for RGB dataset on a 3 layer architecture.\relax }}{3}}
\newlabel{fig:arch2}{{3}{3}}
\citation{Adam}
\citation{Ada}
\citation{AdaD}
\citation{grey2}
\citation{Flip}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Classification accuracy for different Learning rate update rules trained on a 3 layer network of 200 hidden units per layer.\relax }}{4}}
\newlabel{fig:opt}{{4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Classification accuracy when applying different functions for data augmentation.\relax }}{5}}
\newlabel{fig:aug}{{5}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Effects of L2 and dropout regularization \relax }}{6}}
\newlabel{fig:reg}{{6}{6}}
\bibcite{grey1}{1}
\bibcite{grey2}{2}
\bibcite{grey3}{3}
\bibcite{Ada}{4}
\bibcite{Adam}{5}
\bibcite{AdaD}{6}
\bibcite{Flip}{7}
